{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcc2c6f",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc444d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "C:\\Users\\Ivan\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск обучения на cuda, размерность SMM-представления на агента: (72, 96, 4), размерность действия: 19. Используется 5 независимых CNN, 1 оптимизатор.\n",
      "Глобальные шаги: 16384. Сбор данных...\n",
      "Глобальные шаги: 16384, среднее вознаграждение за батч (команды): 0.0000, среднее кол-во очков: 0 / 0, 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gfootball.env as football_env\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "from models import AgentNN\n",
    "from ippo_method import compute_gae, ppo_update\n",
    "from envs_create import make_gfootball_env\n",
    "\n",
    "\n",
    "res_dir = \"PPO_academy_5_vs_5\"\n",
    "\n",
    "if not os.path.exists(res_dir):\n",
    "    os.mkdir(res_dir)\n",
    "\n",
    "# 1. Конфигурация и гиперпараметры\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_ENVS = 12\n",
    "NUM_AGENTS = 5\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPS = 0.05\n",
    "PPO_EPOCHS = 4\n",
    "MINIBATCH_SIZE = 2048\n",
    "LR = 5e-5\n",
    "TOTAL_FRAMES = 10_000_000\n",
    "FRAMES_PER_BATCH = 8192*2 \n",
    "\n",
    "\n",
    "# 2. Цикл обучения\n",
    "if __name__ == '__main__':\n",
    "    temp_env = make_gfootball_env(NUM_AGENTS)\n",
    "    temp_obs = temp_env.reset() \n",
    "    obs_shape = temp_obs.shape[1:] # (72, 96, 4)\n",
    "    temp_env.close()\n",
    "    action_dim = 19 \n",
    "    envs = [make_gfootball_env(NUM_AGENTS) for _ in range(NUM_ENVS)]\n",
    "    \n",
    "    agents = [AgentNN(obs_shape, action_dim).to(DEVICE) for _ in range(NUM_AGENTS)]\n",
    "    all_agent_parameters = []\n",
    "    for agent in agents:\n",
    "        all_agent_parameters.extend(list(agent.parameters()))\n",
    "        \n",
    "    optimizer = optim.Adam(all_agent_parameters, lr=LR, weight_decay=1e-5)#, eps=1e-5)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    print(f\"Запуск обучения на {DEVICE}, размерность SMM-представления на агента: {obs_shape}, размерность действия: {action_dim}. Используется 5 независимых CNN, 1 оптимизатор.\")\n",
    "    \n",
    "    \n",
    "    scores_team_ema = 0\n",
    "    ema_coef = 0.98\n",
    "    \n",
    "    scores_team, num_done = 0, 0\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    N_steps = 0\n",
    "    \n",
    "    while global_step < TOTAL_FRAMES:\n",
    "        \n",
    "        mb_data_agentwise = [{'obs': [], 'actions': [], 'logprobs': [], 'rewards': [], 'values': [], 'dones': []} for _ in range(NUM_AGENTS)]\n",
    "        current_obs = [env.reset() for env in envs] # Список из 4 массивов (5, 72, 96, 4)\n",
    "        steps_per_env_agent = FRAMES_PER_BATCH // (NUM_ENVS * NUM_AGENTS)\n",
    "\n",
    "        for _ in range(steps_per_env_agent):\n",
    "            obs_stack = np.stack(current_obs, axis=0) # (NUM_ENVS, NUM_AGENTS, 72, 96, 4)\n",
    "            actions_np = np.zeros((NUM_ENVS, NUM_AGENTS), dtype=int)\n",
    "            values_np = np.zeros((NUM_ENVS, NUM_AGENTS), dtype=np.float32)\n",
    "            logprobs_np = np.zeros((NUM_ENVS, NUM_AGENTS), dtype=np.float32)\n",
    "\n",
    "            for agent_idx in range(NUM_AGENTS):\n",
    "                # Выбор среза данных по агенту\n",
    "                agent_obs = torch.tensor(obs_stack[:, agent_idx, :, :, :], dtype=torch.float32).to(DEVICE) # (NUM_ENVS, 72, 96, 4)\n",
    "                with torch.no_grad():\n",
    "                    action, logprob, _, value = agents[agent_idx].get_action_and_value(agent_obs)\n",
    "                actions_np[:, agent_idx] = action.cpu().numpy().astype(int)\n",
    "                values_np[:, agent_idx] = value.squeeze().cpu().numpy()\n",
    "                logprobs_np[:, agent_idx] = logprob.cpu().numpy()\n",
    "\n",
    "            step_results = [env.step(a) for env, a in zip(envs, actions_np)]\n",
    "            next_obs_list, rewards_list_step, dones_list_step, info_list_step = zip(*step_results)\n",
    "            \n",
    "            for to_reset, done_val, score_val in zip(envs, dones_list_step, info_list_step):\n",
    "                if done_val: \n",
    "                    scores_team_ema = ema_coef*scores_team_ema + (1-ema_coef)*score_val[\"score_reward\"]\n",
    "                    num_done += 1\n",
    "                    \n",
    "                    scores_team += score_val[\"score_reward\"]\n",
    "                    to_reset.reset()\n",
    "\n",
    "            rewards_per_agent_step = np.repeat(np.array(rewards_list_step)[:, np.newaxis], NUM_AGENTS, axis=1) \n",
    "            dones_per_agent_step = np.repeat(np.array(dones_list_step)[:, np.newaxis], NUM_AGENTS, axis=1) \n",
    "\n",
    "            for agent_idx in range(NUM_AGENTS):\n",
    "                mb_data_agentwise[agent_idx]['obs'].append(obs_stack[:, agent_idx, :, :, :])\n",
    "                mb_data_agentwise[agent_idx]['actions'].append(actions_np[:, agent_idx])\n",
    "                mb_data_agentwise[agent_idx]['logprobs'].append(logprobs_np[:, agent_idx])\n",
    "                mb_data_agentwise[agent_idx]['rewards'].append(rewards_per_agent_step[:, agent_idx]) \n",
    "                mb_data_agentwise[agent_idx]['values'].append(values_np[:, agent_idx])\n",
    "                mb_data_agentwise[agent_idx]['dones'].append(dones_per_agent_step[:, agent_idx])\n",
    "            current_obs = list(next_obs_list)\n",
    "\n",
    "        global_step += FRAMES_PER_BATCH\n",
    "        print(f\"Глобальные шаги: {global_step}. Сбор данных...\")\n",
    "\n",
    "        next_values_per_agent_structured = np.zeros((NUM_ENVS, NUM_AGENTS), dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            next_obs_stack = np.stack(current_obs, axis=0)\n",
    "            for agent_idx in range(NUM_AGENTS):\n",
    "                agent_next_obs = torch.tensor(next_obs_stack[:, agent_idx, :, :, :], dtype=torch.float32).to(DEVICE)\n",
    "                next_val = agents[agent_idx].get_value(agent_next_obs).squeeze().cpu().numpy()\n",
    "                next_values_per_agent_structured[:, agent_idx] = next_val\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        avg_rewards_list = []\n",
    "\n",
    "        for agent_idx in range(NUM_AGENTS):\n",
    "            \n",
    "            agent_data = mb_data_agentwise[agent_idx]\n",
    "\n",
    "            # Конкатенация списков массивов\n",
    "            all_obs = np.concatenate(agent_data['obs'])\n",
    "            all_actions = np.concatenate(agent_data['actions'])\n",
    "            all_logprobs = np.concatenate(agent_data['logprobs'])\n",
    "            all_rewards_flat = np.concatenate(agent_data['rewards']).flatten()\n",
    "            all_values_flat = np.concatenate(agent_data['values']).flatten()\n",
    "            all_dones_flat = np.concatenate(agent_data['dones']).flatten()\n",
    "            \n",
    "            avg_rewards_list.append(np.sum(all_rewards_flat))\n",
    "\n",
    "            agent_advantages = []\n",
    "            agent_returns = []\n",
    "            T = steps_per_env_agent\n",
    "            for env_idx in range(NUM_ENVS):\n",
    "                env_rewards = all_rewards_flat[env_idx * T : (env_idx + 1) * T]\n",
    "                env_dones = all_dones_flat[env_idx * T : (env_idx + 1) * T]\n",
    "                env_values = all_values_flat[env_idx * T : (env_idx + 1) * T]\n",
    "                next_val_scalar = next_values_per_agent_structured[env_idx, agent_idx]\n",
    "                \n",
    "                adv, ret = compute_gae(next_val_scalar, env_rewards, env_dones, env_values, GAMMA, GAE_LAMBDA)\n",
    "                agent_advantages.append(adv)\n",
    "                agent_returns.append(ret)\n",
    "            \n",
    "            all_advantages_flat = np.concatenate(agent_advantages)\n",
    "            all_returns_flat = np.concatenate(agent_returns)\n",
    "\n",
    "            batch_data = {\n",
    "                'obs': torch.tensor(all_obs, dtype=torch.float32).to(DEVICE),\n",
    "                'actions': torch.tensor(all_actions, dtype=torch.long).to(DEVICE),\n",
    "                'logprobs': torch.tensor(all_logprobs, dtype=torch.float32).to(DEVICE),\n",
    "                'returns': torch.tensor(all_returns_flat, dtype=torch.float32).to(DEVICE),\n",
    "                'advantages': torch.tensor(all_advantages_flat, dtype=torch.float32).to(DEVICE),\n",
    "            }\n",
    "            \n",
    "            if np.random.uniform(0, 1) < 0.5: continue # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            \n",
    "            ppo_update(agents[agent_idx], optimizer=optimizer, batch_data=batch_data, \n",
    "                       PPO_EPOCHS = PPO_EPOCHS, MINIBATCH_SIZE = MINIBATCH_SIZE, CLIP_EPS = CLIP_EPS) \n",
    "\n",
    "        nn.utils.clip_grad_norm_(all_agent_parameters, max_norm=0.5) ## !!!!!!!!!!!!!!! WAS 0.5\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_rewards_across_all_agents = np.mean(avg_rewards_list)\n",
    "        print(f\"Глобальные шаги: {global_step}, среднее вознаграждение за батч (команды): {avg_rewards_across_all_agents:.4f}, среднее кол-во очков: {scores_team} / {num_done}, {scores_team_ema:.4f}\") #{avg_rewards_across_all_agents:.4f}\")\n",
    "        \n",
    "        \n",
    "        N_steps += 1\n",
    "        \n",
    "        if N_steps % 20 == 0: #409600 == 0:\n",
    "             torch.save({f'agent_{i}': agents[i].state_dict() for i in range(NUM_AGENTS)}, \n",
    "                        f\"{res_dir}/ppo_gfootball_checkpoint_{global_step}.pt\")\n",
    "\n",
    "    for env in envs:\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c34265",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({f'agent_{i}': agents[i].state_dict() for i in range(NUM_AGENTS)}, \n",
    "                        f\"{res_dir}/ppo_gfootball_checkpoint_{global_step}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d10f9",
   "metadata": {},
   "source": [
    "## Drawing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully from ../PPO_academy_5_vs_5_TEST3/ppo_gfootball_checkpoint_2949120.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Ivan\\anaconda3\\envs\\myenv\\lib\\site-packages\\gym\\core.py:51: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test episode and manually collecting SMM frames...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gfootball.env as football_env\n",
    "from typing import List\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from models import AgentNN\n",
    "from envs_create import make_gfootball_env\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = '../PPO_academy_5_vs_5_TEST3/ppo_gfootball_checkpoint_2293760.pt'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_AGENTS = 5 \n",
    "OBS_SHAPE = (72, 96, 4) \n",
    "ACTION_DIM = 19 \n",
    "VIDEO_FILENAME = \"gfootball_replay_manual3v1.mp4\"\n",
    "\n",
    "def run_test_episode_and_record(model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=DEVICE)\n",
    "        agents = [AgentNN(OBS_SHAPE, ACTION_DIM).to(DEVICE) for _ in range(NUM_AGENTS)]\n",
    "        for i in range(NUM_AGENTS):\n",
    "            agents[i].load_state_dict(checkpoint[f'agent_{i}'])\n",
    "            agents[i].eval() \n",
    "        print(f\"Models loaded successfully from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return\n",
    "\n",
    "    test_env = football_env.create_environment(\n",
    "        env_name='5_vs_5',\n",
    "        stacked=False,\n",
    "        representation='extracted', \n",
    "        render=False,\n",
    "        number_of_left_players_agent_controls=NUM_AGENTS,\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting test episode and manually collecting SMM frames...\")\n",
    "\n",
    "    frames = []\n",
    "    obs_list = []\n",
    "    with torch.no_grad():\n",
    "        obs = test_env.reset() \n",
    "        done = False\n",
    "        while not done:\n",
    "            frames.append(test_env.render(mode='rgb_array')) \n",
    "            \n",
    "            actions_np = np.zeros((NUM_AGENTS,), dtype=int)\n",
    "            for agent_idx in range(NUM_AGENTS):\n",
    "                agent_obs = torch.tensor(obs[agent_idx:agent_idx+1], dtype=torch.float32).to(DEVICE)\n",
    "                action, _, _, _ = agents[agent_idx].get_action_and_value(agent_obs)\n",
    "                actions_np[agent_idx] = action.cpu().numpy().item()\n",
    "\n",
    "            obs, reward, done_bool, info = test_env.step(actions_np)\n",
    "            \n",
    "            obs_list.append(obs)\n",
    "            \n",
    "            if done_bool:\n",
    "                done = True\n",
    "        \n",
    "    test_env.close()\n",
    "\n",
    "    return frames, obs_list\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    frames, obs_list = run_test_episode_and_record(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342b2d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 3001 frames to gfootball_replay_manual5v5.mp4 using imageio...\n",
      "Successfully saved video to C:\\Users\\Ivan\\Desktop\\Текущие статьи ИТМО\\1. NN_communication\\Bisheaf autoencoder\\gfootball_replay_manual5v5.mp4\n"
     ]
    }
   ],
   "source": [
    "VIDEO_FILENAME = \"gfootball_replay_manual5v5.mp4\"\n",
    "\n",
    "\n",
    "print(f\"Saving {len(frames)} frames to {VIDEO_FILENAME} using imageio...\")\n",
    "\n",
    "imageio.mimsave(VIDEO_FILENAME, frames[:1800], fps=30)  #VIDEO_FILENAME\n",
    "\n",
    "print(f\"Successfully saved video to {os.path.abspath(VIDEO_FILENAME)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33058a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
